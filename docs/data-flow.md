# Data Flow

This document provides a detailed explanation of how data flows through the system from scraping reviews to generating Excel reports.

## Table of Contents

1. [Overview](#overview)
2. [Phase 1: Data Collection](#phase-1-data-collection)
3. [Phase 2: Topic Extraction](#phase-2-topic-extraction)
4. [Phase 3: Topic Consolidation](#phase-3-topic-consolidation)
5. [Phase 4: Trend Analysis](#phase-4-trend-analysis)
6. [Phase 5: Report Generation](#phase-5-report-generation)
7. [Optimization Strategies](#optimization-strategies)

---

## Overview

The system processes data through five sequential phases:

```
Reviews â†’ Topics â†’ Canonical Topics â†’ Counts â†’ Excel Report
  (5k)    (20k)         (18)          (18Ã—30)     (.xlsx)
```

**Key Metrics** (for 30-day analysis of Swiggy):
- **Input**: ~5,000 reviews from Play Store
- **Extracted**: ~20,000 raw topics
- **Consolidated**: ~18 canonical topics
- **Output**: 18Ã—30 Excel matrix + charts

**Processing Time**:
- With cache: ~3-5 minutes
- Without cache: ~10-15 minutes

---

## Phase 1: Data Collection

### Input
- App package ID (e.g., `in.swiggy.android`)
- Date range (start_date â†’ end_date)

### Process

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Data Collection Flow                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. Check Cache
   â”œâ”€ Cache exists & fresh (<24h old)?
   â”‚  â”œâ”€ Yes â†’ Load from cache/in.swiggy.android/reviews_cache.json
   â”‚  â””â”€ No  â†’ Continue to step 2
   â”‚
2. Fetch from Play Store
   â”œâ”€ Use google-play-scraper library
   â”œâ”€ Sort by NEWEST
   â”œâ”€ Fetch in batches of 500
   â”‚  â”œâ”€ Batch 1: Reviews 1-500
   â”‚  â”œâ”€ Batch 2: Reviews 501-1000
   â”‚  â””â”€ ... continue until past start_date (SMART STOPPING)
   â”‚
3. Save to Cache
   â”œâ”€ Convert datetime to ISO strings
   â”œâ”€ Save as JSON to cache/
   â””â”€ Log cache stats
   â”‚
4. Filter by Date Range
   â”œâ”€ Keep only reviews where: start_date â‰¤ review.at â‰¤ end_date
   â””â”€ Organize by date string (YYYY-MM-DD)
```

### Output

```python
{
  "2024-12-25": [
    {
      "reviewId": "abc123",
      "userName": "John Doe",
      "content": "Delivery was 2 hours late, food was cold",
      "score": 1,
      "at": datetime(2024, 12, 25, 14, 30),
      ...
    },
    {
      "reviewId": "def456",
      "content": "Great service, delivery partner was very friendly",
      "score": 5,
      "at": datetime(2024, 12, 25, 15, 45),
      ...
    }
    # ... more reviews for this date
  ],
  "2024-12-24": [ ... ],
  ...
}
```

### Data Transformations

```
Play Store API Response
  â†“
[Datetime Parsing]
  â†“
Cached JSON (datetime â†’ ISO string)
  â†“
[Load from Cache]
  â†“
Python dict (ISO string â†’ datetime)
  â†“
[Filter by Date]
  â†“
Reviews organized by date
```

### Cache Structure

```json
{
  "app_id": "in.swiggy.android",
  "last_update": "2024-12-25T14:30:00",
  "total_reviews": 5243,
  "reviews": [
    {
      "reviewId": "abc123",
      "userName": "John Doe",
      "userImage": "https://...",
      "content": "Delivery was 2 hours late...",
      "score": 1,
      "thumbsUpCount": 5,
      "reviewCreatedVersion": "4.32.0",
      "at": "2024-12-25T14:30:00",
      "replyContent": null,
      "repliedAt": null
    }
  ]
}
```

### Smart Stopping Example

```
Timeline (sorted NEWEST first):
  Dec 25 â”€â”€â–º Dec 24 â”€â”€â–º Dec 23 â”€â”€â–º ... â”€â”€â–º Nov 26 â”€â”€â–º Nov 25 (STOP!)
                                             â†‘
                                        start_date

Instead of fetching ALL reviews:
  âœ“ Fetch Dec 25 â†’ Nov 26 (30 days needed)
  âœ— Skip Nov 25 â†’ Ancient (not needed)

Savings: 80-90% fewer API calls
```

---

## Phase 2: Topic Extraction

### Input
```python
{
  "2024-12-25": [review1, review2, ...],
  "2024-12-24": [...],
  ...
}
```

### Process

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Topic Extraction Flow                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

For each date:
  1. Batch Reviews
     â”œâ”€ Group into batches of 20 reviews
     â”œâ”€ Example: 150 reviews â†’ 8 batches
     â”‚
  2. Parallel Processing (8 workers)
     â”œâ”€ Worker 1: Batch 0 (reviews 0-19)
     â”œâ”€ Worker 2: Batch 1 (reviews 20-39)
     â”œâ”€ Worker 3: Batch 2 (reviews 40-59)
     â”œâ”€ ...
     â””â”€ Worker 8: Batch 7 (reviews 140-159)
     â”‚
  3. For each batch:
     â”œâ”€ Build prompt with 20 reviews
     â”œâ”€ Call LLM API (single call for 20 reviews)
     â”œâ”€ Parse JSON response
     â””â”€ Extract topics for each review
     â”‚
  4. Collect Results
     â”œâ”€ Merge topics from all batches
     â””â”€ Flatten to list of topic strings
```

### LLM Prompt Example

**Input to LLM**:
```
Extract ALL topics from these app reviews. Prioritize HIGH RECALL.

Include context in topic names (e.g., "delivery partner rude" not just "rude").
Detect sarcasm: "Great job delivering cold food" â†’ "food delivered cold"
Return max 5 most important topics per review.

Reviews:
0. Delivery was 2 hours late, food was cold
1. Great service today, delivery partner was very friendly
2. App keeps crashing when I try to checkout

Output JSON object ONLY (no markdown, just raw JSON):
{"reviews": [
  {"review_id": "0", "topics": [{"topic": "...", "category": "issue|request|feedback"}]},
  {"review_id": "1", "topics": [...]},
  ...
]}
```

**LLM Response**:
```json
{
  "reviews": [
    {
      "review_id": "0",
      "topics": [
        {"topic": "delivery delay 2 hours", "category": "issue"},
        {"topic": "food delivered cold", "category": "issue"}
      ]
    },
    {
      "review_id": "1",
      "topics": [
        {"topic": "positive feedback", "category": "feedback"},
        {"topic": "delivery partner friendly", "category": "feedback"}
      ]
    },
    {
      "review_id": "2",
      "topics": [
        {"topic": "app crashes at checkout", "category": "issue"}
      ]
    }
  ]
}
```

### Output

```python
{
  "2024-12-25": [
    "delivery delay 2 hours",
    "food delivered cold",
    "positive feedback",
    "delivery partner friendly",
    "app crashes at checkout",
    # ... more topics from other reviews
  ],
  "2024-12-24": [...],
  ...
}
```

### Parallelization Strategy

```
Sequential Processing:
  Review 1 â†’ LLM â†’ Wait â†’ Review 2 â†’ LLM â†’ Wait â†’ ...
  Time: 5000 reviews Ã— 2 sec = 10,000 seconds (2.7 hours)

Batch Processing:
  Batch 1 (20 reviews) â†’ LLM â†’ Wait â†’ Batch 2 (20 reviews) â†’ ...
  Time: 250 batches Ã— 3 sec = 750 seconds (12.5 min)

Parallel Batch Processing:
  â”Œâ”€ Batch 1 â†’ LLM â”€â”
  â”œâ”€ Batch 2 â†’ LLM â”€â”¤
  â”œâ”€ Batch 3 â†’ LLM â”€â”¤ â† 8 workers
  â”œâ”€ ...           â”€â”¤   running in
  â””â”€ Batch 8 â†’ LLM â”€â”˜   parallel
  Time: 250 batches Ã· 8 workers Ã— 3 sec = 94 seconds (1.5 min)

Speed: 64x faster than sequential!
```

### Topic Categorization

**Issue**: Problems users report
```
"delivery delay", "food cold", "app crashes"
```

**Request**: Features users want
```
"10 minute delivery", "24/7 service", "express delivery"
```

**Feedback**: General comments
```
"positive feedback", "good service", "pricing concerns"
```

---

## Phase 3: Topic Consolidation

### Input
```python
[
  "delivery delay 2 hours",
  "delivery delayed",
  "late delivery",
  "food delivered cold",
  "cold food",
  "food not hot",
  "app crashes at checkout",
  "app freezing",
  ...
]
# ~20,000 topics from ~5,000 reviews
```

### Process

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Topic Consolidation Flow                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. Remove Duplicates
   â”œâ”€ ["delivery delay", "delivery delay", "food cold"]
   â””â”€ â†’ ["delivery delay", "food cold"]
   â”‚
2. Normalize Text
   â”œâ”€ "The delivery guy was very rude" â†’ "delivery partner rude"
   â”œâ”€ Remove: articles (a, the), intensifiers (very), tense markers
   â””â”€ Standardize: "delivery guy" â†’ "delivery partner"
   â”‚
3. Group Exact Matches
   â”œâ”€ After normalization, group identical topics
   â””â”€ 20,000 â†’ 342 unique normalized topics
   â”‚
4. LLM-Based Semantic Grouping
   â”œâ”€ Send 342 unique topics to LLM (Consolidation model)
   â”œâ”€ LLM identifies semantic similarity
   â”œâ”€ Target: 15-25 canonical topics
   â””â”€ Returns mapping: canonical â†’ [variations]
   â”‚
5. Build Canonical Mapping
   â”œâ”€ For each canonical topic
   â”œâ”€ Expand to include all normalized variations
   â””â”€ Create reverse mapping: variation â†’ canonical
```

### LLM Consolidation Prompt

**Input to LLM**:
```
You are consolidating topics from app reviews. BE EXTREMELY AGGRESSIVE -
aim for 15-25 final topics MAXIMUM.

Here are 342 extracted topics:
- delivery delay 2 hours
- delivery delayed 2 hours
- 2 hour delivery wait
- delivery extremely delayed
- late delivery
- food cold
- cold food delivered
- food not hot
- ...

CRITICAL RULES - MERGE EVERYTHING SIMILAR:
1. ALL positive feedback â†’ "Positive feedback"
2. ALL negative delivery partner behavior â†’ "Delivery partner unprofessional"
3. ALL delivery delays â†’ "Delivery delay"
4. ALL food temperature issues â†’ "Food temperature issues"
5. ALL app crashes/freezes â†’ "App crashes/freezes"
...

OUTPUT (JSON only, no markdown):
{
  "canonical_topics": [
    {
      "canonical_name": "Delivery delay",
      "variations": ["delivery delay 2 hours", "late delivery", ...]
    },
    ...
  ]
}
```

**LLM Response**:
```json
{
  "canonical_topics": [
    {
      "canonical_name": "Delivery delay",
      "variations": [
        "delivery delay 2 hours",
        "delivery delayed 2 hours",
        "2 hour delivery wait",
        "late delivery",
        "delivery extremely delayed"
      ]
    },
    {
      "canonical_name": "Food temperature issues",
      "variations": [
        "food cold",
        "cold food delivered",
        "food not hot",
        "lukewarm food"
      ]
    },
    {
      "canonical_name": "App crashes/freezes",
      "variations": [
        "app crashes at checkout",
        "app freezing",
        "app stuck",
        "app not responding"
      ]
    }
  ]
}
```

### Output

```python
{
  "Delivery delay": [
    "delivery delay 2 hours",
    "delivery delayed 2 hours",
    "2 hour delivery wait",
    "late delivery",
    "delivery extremely delayed"
  ],
  "Food temperature issues": [
    "food cold",
    "cold food delivered",
    "food not hot",
    "lukewarm food"
  ],
  "App crashes/freezes": [
    "app crashes at checkout",
    "app freezing",
    "app stuck",
    "app not responding"
  ],
  # ... 15 more canonical topics
}
```

### Consolidation Funnel

```
20,000 raw topics (from extraction)
    â†“
 [Remove duplicates]
    â†“
 342 unique topics
    â†“
 [Normalize text]
    â†“
 187 normalized groups
    â†“
 [LLM semantic grouping]
    â†“
  18 canonical topics

Reduction: 1111x (20,000 â†’ 18)
```

### Normalization Examples

```
Before:                         After:
"The delivery guy was rude"  â†’  "delivery partner rude"
"Delivery guy very rude"     â†’  "delivery partner rude"
"Rude delivery partner"      â†’  "delivery partner rude"
"Impolite delivery person"   â†’  "delivery partner rude"  (fuzzy match)

All map to: "Delivery partner unprofessional"
```

---

## Phase 4: Trend Analysis

### Input

**Extracted Topics by Date**:
```python
{
  "2024-12-25": ["delivery delay 2 hours", "food cold", ...],
  "2024-12-24": ["late delivery", "cold food", ...],
  ...
}
```

**Canonical Mapping**:
```python
{
  "Delivery delay": ["delivery delay 2 hours", "late delivery", ...],
  "Food temperature issues": ["food cold", "cold food", ...],
  ...
}
```

### Process

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Trend Analysis Flow                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. Build Reverse Mapping
   â”œâ”€ variation â†’ canonical lookup table
   â”œâ”€ "delivery delay 2 hours" â†’ "Delivery delay"
   â”œâ”€ "late delivery" â†’ "Delivery delay"
   â””â”€ "food cold" â†’ "Food temperature issues"
   â”‚
2. Map & Count
   â”œâ”€ For each date:
   â”‚  â”œâ”€ For each extracted topic:
   â”‚  â”‚  â”œâ”€ Look up canonical version
   â”‚  â”‚  â””â”€ Increment count for (date, canonical)
   â”‚
3. Handle Unmapped Topics
   â”œâ”€ Topics not in canonical mapping
   â”œâ”€ Try fuzzy matching (contains/substring)
   â”œâ”€ Track for validation report
   â””â”€ Suggest best canonical match
   â”‚
4. Build Count Matrix
   â””â”€ canonical_counts[date][topic] = count
```

### Mapping Example

```
Date: 2024-12-25
Extracted topics:
  1. "delivery delay 2 hours"  â†’  Delivery delay
  2. "food cold"                â†’  Food temperature issues
  3. "late delivery"            â†’  Delivery delay
  4. "app crashes at checkout"  â†’  App crashes/freezes
  5. "delivery delay"           â†’  Delivery delay

Counts for 2024-12-25:
  - Delivery delay: 3
  - Food temperature issues: 1
  - App crashes/freezes: 1
```

### Output

```python
{
  "2024-12-25": {
    "Delivery delay": 12,
    "Food temperature issues": 8,
    "Positive feedback": 25,
    "App crashes/freezes": 3,
    "Delivery partner unprofessional": 6,
    ...
  },
  "2024-12-24": {
    "Delivery delay": 15,
    "Food temperature issues": 7,
    ...
  },
  ...
}
```

### Unmapped Topics Handling

```
Unmapped: "delivery guy extremely rude"

Fuzzy matching:
  Check if contains "delivery" + "rude"
  â†’ Best match: "Delivery partner unprofessional"

Suggest in report:
  "delivery guy extremely rude" â†’ "Delivery partner unprofessional"
```

---

## Phase 5: Report Generation

### Input

```python
canonical_counts = {
  "2024-12-25": {"Delivery delay": 12, "Food issues": 8, ...},
  "2024-12-24": {"Delivery delay": 15, ...},
  ...
}
```

### Process

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Report Generation Flow                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. Calculate Date Range
   â”œâ”€ start_date = target_date - 29 days
   â”œâ”€ Generate list: [Dec 1, Dec 2, ..., Dec 30]
   â”‚
2. Collect All Topics
   â”œâ”€ Union of all topics across all dates
   â”œâ”€ Sort: canonical first, unmapped last
   â”‚
3. Build Matrix
   â”œâ”€ Rows: Topics
   â”œâ”€ Columns: Dates
   â”œâ”€ Cells: Counts (0 if topic not mentioned that day)
   â”‚
4. Create Excel Workbook
   â”œâ”€ Add headers (blue background)
   â”œâ”€ Add data rows (alternating colors)
   â”œâ”€ Highlight unmapped topics (yellow)
   â”œâ”€ Auto-size columns
   â””â”€ Add legend
   â”‚
5. Validation Report
   â”œâ”€ Check: expected topics vs actual
   â”œâ”€ List unmapped topics with suggestions
   â””â”€ Print to console
   â”‚
6. Save File
   â””â”€ output/<app>_trend_report_<date>.xlsx
```

### Excel Layout

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”
â”‚ Topic                   â”‚Dec 1 â”‚Dec 2 â”‚Dec 3 â”‚ ... â”‚Dec 30â”‚  â† Header (Blue)
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¤
â”‚ Delivery delay          â”‚  12  â”‚  15  â”‚  18  â”‚ ... â”‚  20  â”‚  â† Row 1 (Light blue)
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¤
â”‚ Food temperature issues â”‚   8  â”‚   7  â”‚   6  â”‚ ... â”‚  11  â”‚  â† Row 2 (White)
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¤
â”‚ Positive feedback       â”‚  25  â”‚  28  â”‚  30  â”‚ ... â”‚  32  â”‚  â† Row 3 (Light blue)
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¤
â”‚ ...                     â”‚  ... â”‚  ... â”‚  ... â”‚ ... â”‚  ... â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”˜

âš ï¸ Yellow-highlighted topics are unmapped (not in canonical list)
```

### Formatting Rules

```python
# Header row
fill = Blue (#366092)
font = Bold, White
alignment = Center

# Data rows
if row % 2 == 0:
    fill = Light Blue (#E8EFF7)
else:
    fill = White

# Unmapped topics (column A only)
if topic not in canonical_mapping:
    fill = Yellow (#FFF4CC)
    font = Italic

# Column widths
Column A (Topic): 40 characters
Columns B-AE (Dates): 12 characters
```

### Validation Output

```
ğŸ“Š VALIDATION REPORT:
  Expected canonical topics: 18
  Topics appearing in Excel: 20
  âš ï¸  WARNING: Mismatch detected (20 vs 18)

  âŒ Topics in Excel but NOT in canonical mapping (2):
    - 'delivery guy extremely rude' (suggested: 'Delivery partner unprofessional')
    - 'food spoiled badly' (suggested: 'Food freshness issues')

  âœ… All 18 canonical topics appear in data
```

---

## Optimization Strategies

### 1. Caching (100x speedup)

```
Without cache:
  Scrape 5000 reviews: 5-10 minutes

With cache:
  Load 5000 reviews: 2-3 seconds

Speedup: 100-200x
```

### 2. Batch Processing (20x speedup)

```
Individual requests:
  5000 reviews Ã— 1 request = 5000 requests
  Time: ~30 minutes (with API limits)

Batch requests (20 per batch):
  5000 reviews Ã· 20 = 250 requests
  Time: ~8 minutes

Speedup: ~4x
```

### 3. Parallel Processing (8x speedup)

```
Sequential batches:
  250 batches Ã— 3 sec = 750 seconds

Parallel (8 workers):
  250 batches Ã· 8 workers Ã— 3 sec = 94 seconds

Speedup: 8x
```

### 4. Combined Optimization

```
Base (sequential, no cache):
  Scrape: 10 min + Extract: 30 min = 40 minutes

Optimized (cache + batch + parallel):
  Load cache: 3 sec + Extract: 90 sec = 93 seconds

Total speedup: 26x
```

### 5. Model Selection

```
Extraction (bulk):
  Model: qwen2.5:32b (smaller, faster)
  Frequency: 250 calls
  Time per call: 2-3 sec

Consolidation (once):
  Model: llama3.1:70b (larger, better quality)
  Frequency: 1 call
  Time per call: 5-10 sec

Strategy: Use fast model for repetitive tasks,
          high-quality model for critical once-off tasks
```

---

## Data Flow Summary

### End-to-End Example

**Input**:
```
App: in.swiggy.android
Date Range: Dec 1 - Dec 30, 2024
```

**Flow**:
```
1. Scrape â†’ 5,243 reviews
2. Extract â†’ 20,972 raw topics
3. Consolidate â†’ 18 canonical topics
4. Map â†’ 30 Ã— 18 count matrix
5. Generate â†’ Excel file (18 rows Ã— 31 columns)
```

**Output**:
```
output/swiggy_trend_report_2024-12-30.xlsx
Size: ~50 KB
Contains:
  - 18 topics
  - 30 days of data
  - 540 cells with counts
  - Formatting + legends
```

### Performance Metrics

| Metric | Value |
|--------|-------|
| Reviews processed | 5,243 |
| Topics extracted | 20,972 |
| Canonical topics | 18 |
| API calls (with batching) | ~262 |
| Processing time (cached) | ~3 min |
| Processing time (no cache) | ~12 min |
| Output file size | ~50 KB |
| Data reduction ratio | 1165:1 |

---

## See Also

- [Architecture](architecture.md) - System design
- [User Guide](user-guide.md) - How to use
- [API Reference](api-reference.md) - Code documentation
- [Troubleshooting](troubleshooting.md) - Common issues
